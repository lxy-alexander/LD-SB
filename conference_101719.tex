\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs} 
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{How Network Depth Affects Low-Dimensional Simplicity Bias\\}

\author{\IEEEauthorblockN{Alexander Li\textsuperscript{*}}
\IEEEauthorblockA{\textit{Binghamton University} \\
xli49@binghamton.edu}
\and
\IEEEauthorblockN{Michael Bronikowski\textsuperscript{*}}
\IEEEauthorblockA{\textit{Binghamton University} \\
mbronik1@binghamton.edu}
\thanks{\textsuperscript{*}Equal contribution}
}


\maketitle

\begin{abstract}
Neural networks exhibit simplicity bias (SB): they rely on the simplest predictive features even when more robust alternatives exist. Morwani et al.\ formalized this through Low-Dimensional Simplicity Bias (LD-SB), proving that one-hidden-layer networks collapse onto low-dimensional subspaces. However, their analysis focused on shallow networks, leaving the effect of depth unexplored.

We investigate how network depth affects LD-SB by training MLPs with 1--50 hidden layers on Waterbirds features and measuring effective rank and projection-mixing metrics. Our results show that depth dramatically increases the dimensionality of the predictive subspace (rank grows from 4.68 to 95.74) while LD-SB metrics remain stable. Comparing rich and lazy training regimes reveals that this depth effect operates through gradient-based feature learning: deeper networks in the rich regime suppress rank collapse, while lazy-regime networks maintain high rank regardless of depth. These findings demonstrate that depth reshapes but does not eliminate simplicity bias.

\end{abstract}


\section{Introduction}

Deep neural networks achieve strong performance across many tasks, yet often fail under distribution shifts or adversarial perturbations \cite{b1}. A growing body of work attributes part of this fragility to \emph{simplicity bias} (SB)—the tendency of neural networks to rely on the easiest predictive features available, even when richer and more robust features exist \cite{b2, b3}. For example, classifiers trained on Waterbirds frequently exploit background correlations (water vs.\ land) instead of learning bird-specific visual features, leading to sharp accuracy drops when these correlations change.

Morwani et al.\ formalize this tendency through the Low-Dimensional Simplicity Bias (LD-SB) framework, showing that one-hidden-layer ReLU networks trained in the rich regime collapse onto a low-dimensional subspace of the input \cite{b4}. This rank collapse explains why models depend heavily on a small set of predictive directions and ignore alternatives.

However, LD-SB has been primarily studied in shallow networks. This leaves a natural architectural question unanswered: \emph{Does increasing network depth mitigate simplicity bias?} Deeper models might distribute feature learning across layers, potentially preventing extreme rank collapse in early layers. On the other hand, simplicity bias may remain a persistent property regardless of depth.

To investigate this, we train multilayer perceptrons (MLPs) with varying depths (1--50 layers) on 2048-dimensional ResNet-50 features from the Waterbirds dataset and measure simplicity bias using the LD-SB projection-mixing metrics. Our goal is to understand whether depth changes the dimensionality of the learned representation or the model's dependence on low-dimensional predictive subspaces. All code used in this study is publicly available at:
\url{https://github.com/lxy-alexander/LD-SB}.



\section{Background and Gap in the Literature}

\subsection{Prior Work on Simplicity Bias}

A substantial body of work studies simplicity bias (SB)—the tendency of neural networks to rely on readily learnable but potentially non-robust features. Much of this research uses synthetic or semi-synthetic datasets where “simple’’ and “complex’’ features can be explicitly controlled. Representative examples include:

\begin{itemize}
    \item \textbf{MNIST--CIFAR concatenation} \cite{b2}, where networks often learn the MNIST component first.
    \item \textbf{Colored MNIST} \cite{b5}, where label-dependent colors induce a strong but spurious predictive signal.
    \item \textbf{Waterbirds} \cite{b6}, which exposes reliance on background cues rather than bird-specific features.
\end{itemize}

Complementary evidence from Geirhos et al.\ shows that ImageNet-trained CNNs prefer local texture over global shape \cite{b3}, highlighting how SB can manifest in naturalistic settings and contribute to fragility under distribution shift.

\subsection{Low-Dimensional Simplicity Bias (LD-SB)}

Morwani et al.\ (2023) formalize SB through the \textbf{Low-Dimensional Simplicity Bias (LD-SB)} framework \cite{b7}, providing both theoretical insights and empirical findings. Their key contributions include:

\begin{enumerate}
    \item A formal characterization of simplicity bias via low-rank projections in feature space,
    \item A proof that one-hidden-layer ReLU networks trained in the rich regime rely on a low-dimensional, linearly separable subspace,
    \item Empirical evidence from Imagenette and Waterbirds indicating that model predictions depend on only a small number of feature directions.
\end{enumerate}

Under LD-SB, for a classifier $f: \mathbb{R}^d \rightarrow \mathcal{Y}$, one identifies a projection matrix $P$ such that
\begin{equation}
f(Px + P^{\perp}x') \approx f(x)
\label{eq:ldsb_core}
\end{equation}
for most input pairs $x,x'$, where $P^{\perp} = I - P$. This expresses that predictions remain stable even when the orthogonal component is replaced, implying that $f$ depends primarily on the low-dimensional subspace spanned by $P$.

\subsection{Gap: Depth and Simplicity Bias}

Despite its general framing, LD-SB has been developed almost entirely for \emph{single hidden-layer} networks. Whether similar behavior persists in deeper architectures—standard in contemporary deep learning—remains unknown.

This gap is significant for several reasons:

\begin{itemize}
    \item Modern neural networks are overwhelmingly deep rather than shallow.
    \item Depth alters representational capacity and optimization dynamics in ways not captured by existing LD-SB theory.
    \item Understanding how depth influences simplicity bias may clarify sources of brittleness and inform architectural design.
\end{itemize}

To our knowledge, no prior work has systematically investigated how network depth affects simplicity bias under the LD-SB formulation. This motivates our study.




\section{Methodology}

\subsection{LD-SB Measurement Framework}

This study follows the feature projection mixing procedure of Morwani et al.\ \cite{b7}.  
Given two feature vectors $x^{(1)}, x^{(2)} \in \mathbb{R}^{2048}$ from different samples, a mixed feature vector is defined as:
\begin{equation}
    x_e = P x^{(1)} + P^{\perp} x^{(2)},
    \label{eq:mixing}
\end{equation}
where $P$ is a rank-$k$ projection matrix and $P^{\perp} = I - P$ denotes its orthogonal complement.

If the model's decision primarily depends on the subspace spanned by $P$, then the mixed vector $x_e$ will receive the same prediction as $x^{(1)}$, since the $P$-subspace components are inherited entirely from $x^{(1)}$. Contributions from $x^{(2)}$ in the orthogonal subspace are expected to have minimal effect.

\subsection{Simplicity Bias Metrics}

Three quantitative metrics are used to assess simplicity bias:

\textbf{Metric 1: $P^{\perp}$-pC (Complementary Projection Consistency)}
\begin{equation}
    P^{\perp}\text{-pC} = \Pr\left[f(x_e) \neq f(x^{(1)})\right].
    \label{eq:pperp_pc}
\end{equation}
This metric measures the probability that the mixed sample's prediction differs from that of $x^{(1)}$. Values near zero indicate that predictions are determined almost entirely by the $P$-subspace.

\textbf{Metric 2: $P$-pC (Primary Projection Consistency)}
\begin{equation}
    P\text{-pC} = \Pr\left[f(x_e) \neq f(x^{(2)})\right].
    \label{eq:p_pc}
\end{equation}
This metric quantifies disagreement with the second sample. Values approaching $100\%$ indicate that the model largely ignores features from the $P^{\perp}$ subspace.

\textbf{Metric 3: $\mathrm{rank}(P)$ (Effective Decision Dimensionality)}

The smallest rank $k$ of the projection matrix $P$ such that $P^{\perp}$-pC $< \epsilon$ (with $\epsilon = 0.01$). Smaller values imply that the model's decisions concentrate on fewer dimensions.

\subsection{Projection Subspace Discovery Algorithm}

The projection matrix $P$ is identified using the following procedure, adapted from Morwani et al.:

\begin{enumerate}
    \item \textbf{PCA-based initialization:}  
    Compute principal components of the feature covariance matrix to obtain candidate high-variance directions.

    \item \textbf{Gradient sensitivity analysis:}  
    Compute the average gradient $\nabla_x f(x)$ over the dataset to identify feature dimensions with high sensitivity.

    \item \textbf{Greedy dimension selection:}  
    Starting with an empty projection, iteratively add the dimension that yields the greatest reduction in $P^{\perp}$-pC, continuing until the threshold is satisfied.
\end{enumerate}

The procedure terminates once $P^{\perp}$-pC $< 0.01$. The final value of $\mathrm{rank}(P)$ is then recorded.


\section{Hypothesis}

Building on the LD-SB framework and prior observations that shallow networks tend to collapse onto a small set of dominant feature directions, we investigate how increasing depth influences simplicity bias in multilayer perceptrons (MLPs). Our working hypothesis is as follows:

\vspace{1em}
\noindent\fbox{%
    \parbox{0.93\columnwidth}{%
        \textbf{HYPOTHESIS:} \textit{Increasing the number of hidden layers in an MLP will alter the strength and manifestation of low-dimensional simplicity bias, such that deeper networks exhibit different effective-rank dynamics and LD-SB characteristics compared to shallow networks.}
    }%
}
\vspace{1em}


\subsection{Rationale}

Several considerations motivate this hypothesis:

\begin{itemize}
    \item \textbf{Compositional nonlinearity:} Each added layer permits richer combinations of input features, enabling the first-layer representation to remain higher-rank.
    \item \textbf{Hierarchical abstraction:} Deeper models may draw on multiple feature dimensions instead of collapsing onto a single dominant direction.
    \item \textbf{Persistent low-dimensional dependence:} Even with higher representational rank, LD-SB theory suggests that prediction-relevant directions may remain low-dimensional, explaining why $P$-pC may saturate.
\end{itemize}

\subsection{Evaluation Criteria}

To examine the effect of network depth on LD-SB, we track two quantities:

\textbf{(1) Effective Rank Dynamics.}  
How the effective rank of the first-layer projection matrix $P$ changes with depth.

\textbf{(2) LD-SB Metrics.}  
How $P$-pC and $P^{\perp}$-pC behave across networks of different depths.


This ensures that observed changes in LD-SB metrics arise from differences in depth rather than discrepancies in training quality.




\section{Experimental Setup}

\subsection{Dataset: Waterbirds}

We use the Waterbirds dataset \cite{b6}, a binary classification task (landbird vs.\ waterbird) with strong background--label correlations. Following Morwani et al.\ \cite{b7}, we extract features using an ImageNet-pretrained ResNet-50 and freeze all backbone parameters. Each image is mapped to a 2048-dimensional feature vector.

\subsection{Feature Extraction}

Each input image $I$ is transformed using a frozen ResNet-50 encoder:
\[
\phi(I) = \mathrm{GAP}(\mathrm{ResNet50}(I)) \in \mathbb{R}^{2048},
\]
where GAP denotes global average pooling. Freezing the encoder ensures that differences in LD-SB arise solely from the MLP architecture.

\subsection{Classifier Architectures}

We train MLP classifiers with varying depth while keeping all other architectural components fixed. Hidden-layer width is set to 100, and all layers use ReLU activations. No batch normalization, dropout, or residual connections are used.

\[
\text{Depths explored: } \{1,\,5,\,10,\,20,\,50\} \text{ layers.}
\]

This extends the original one-hidden-layer setting to study how depth influences LD-SB.

\subsection{Training Configuration}

All models are trained under the \textbf{rich regime} initialization described in \cite{b7}. Training uses 20{,}000 optimization steps with a warmup phase followed by cosine learning-rate decay.

\begin{itemize}
    \item Optimizer: Adam
    \item Batch size: 128
    \item Learning rate: depth-dependent (1.0 for 1-layer, 0.2 for 5-layer, 0.1 for 10-layer, 0.05 for 20-layer, 0.01 for 50-layer)
    \item Loss: cross-entropy
    \item Backbone features: frozen
\end{itemize}

Learning rate is tuned per depth to avoid training collapse and ensure stable feature learning.

\subsection{LD-SB Measurement Protocol}

We evaluate LD-SB using the projection-mixing procedure from Morwani et al.\ \cite{b7}. For each model:

\begin{enumerate}
    \item \textbf{Compute projection matrix $P$.}  
          Let $W$ be the first-layer weight matrix. We compute its SVD and construct  
          \[
          P = V_k V_k^\top,
          \]
          where $k$ is the effective rank of $W$.
    \item \textbf{Sample feature pairs.}  
          Randomly select 1{,}000 feature pairs $(x^{(1)}, x^{(2)})$ from the validation set.
    \item \textbf{Create mixed samples.}  
          Using the decomposition $P + P^\perp = I$, form  
          \[
          x_{\mathrm{mix}} = P x^{(1)} + P^\perp x^{(2)}.
          \]
    \item \textbf{Compute LD-SB metrics.}  
          Measure:
          \begin{itemize}
              \item $P^\perp$-pC: prediction change when replacing the orthogonal component
              \item $P$-pC: prediction change when replacing the projected component
              \item $\mathrm{rank}(P)$: dimensionality of the predictive subspace
          \end{itemize}
\end{enumerate}



\section{Results}

\subsection{Paper Results}
\label{sec:reproduce}

We begin by reproducing the LD-SB phenomena documented by Morwani et al.\ \cite{b7}. This validates that our projection-based measurement implementation aligns with the theoretical and empirical behavior observed in the original work.

\subsubsection{LD-SB in 1-Layer Networks}

The results for 1-layer networks across several datasets in the rich regime are summarized in Table~\ref{tab:rich_ldsb}. These values closely match those reported by Morwani et al.\ and confirm that our LD-SB metrics behave correctly for the canonical shallow-network setting. In particular, the low rank($P$) and high $P$-LC values reflect strong reliance on a low-dimensional predictive subspace.

\begin{table}[htbp]
\centering
\caption{Demonstration of LD-SB in the rich regime.}
\label{tab:rich_ldsb}
\resizebox{\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
Dataset & rank($P$) & $P_{\perp}$-LC ($\downarrow$) & $P$-LC ($\uparrow$) & $P_{\perp}$-pC ($\downarrow$) & $P$-pC ($\uparrow$) \\
\midrule
b-Imagenette & 1 & $28.57 \pm 0.26$ & $92.13 \pm 0.24$ & $6.35 \pm 0.06$ & $47.02 \pm 0.24$ \\
Imagenette   & 10 & $33.64 \pm 1.21$ & $106.29 \pm 0.53$ & $12.04 \pm 0.29$ & $89.88 \pm 0.08$ \\
Waterbirds   & 3 & $25.24 \pm 1.03$ & $102.35 \pm 0.19$ & $6.78 \pm 0.15$ & $35.96 \pm 0.02$ \\
MNIST-CIFAR  & 1 & $38.97 \pm 0.76$ & $101.98 \pm 0.31$ & $5.41 \pm 0.55$ & $45.15 \pm 0.44$ \\
Imagenet     & 150 & $15.78 \pm 0.05$ & $132.05 \pm 0.06$ & $13.05 \pm 0.03$ & $99.76 \pm 0.01$ \\
\bottomrule
\end{tabular}
}
\end{table}

\subsubsection{Effective Rank Evolution}

Figure~\ref{fig:eff_rank_rich} visualizes the evolution of effective rank during training. The rich regime shows rapid rank collapse shortly after training begins, confirming the theoretical predictions of LD-SB. Similar patterns were observed in the lazy regime, though with less dramatic collapse.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.80\linewidth]{rich_rank.png}
\caption{Evolution of effective rank during training in the rich regime across multiple datasets.}
\label{fig:eff_rank_rich}
\end{figure}

\subsection{Depth Experiments}
\label{sec:hypothesis_results}

To evaluate whether network depth influences the manifestation of low-dimensional simplicity bias, we train MLPs with 1, 5, 10, 20, and 50 layers on Waterbirds features and compute LD-SB metrics for each model. Table~\ref{tab:depth_summary} summarizes the key results.

\begin{table}[htbp]
\centering
\caption{Summary of depth-dependent LD-SB behavior in the rich regime. Deeper networks achieve higher effective rank while maintaining strong LD-SB signatures.}
\label{tab:depth_summary}
\begin{tabular}{cccccc}
\toprule
Layers & LR & Val Acc (\%) & Eff. Rank & $P_{\perp}$-pC (\%) & $P$-pC (\%) \\
\midrule
1  & 1.0  & 77.81 & 4.68  & 0.0 & 0.0 \\
5  & 0.2  & 87.07 & 6.90  & 0.2 & 38.0 \\
10 & 0.1  & 87.41 & 24.81 & 0.5 & 38.2 \\
20 & 0.05 & 88.57 & 49.57 & 0.3 & 38.0 \\
50 & 0.01 & 88.49 & 95.74 & 0.5 & 38.4 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key Findings.}
Effective rank grows monotonically with depth: shallow networks (1--5 layers) exhibit strong rank collapse (4.68--6.90), while deeper networks reach 95.74 for 50 layers. Meanwhile, $P_{\perp}$-pC remains below 1\% for all depths, indicating predictions stay insensitive to orthogonal directions. $P$-pC saturates at $\sim$38\% for depths $\geq 5$. Validation accuracy peaks around 10 layers.

\paragraph{Conclusion.}
Depth significantly increases the dimensionality of the predictive subspace (higher $\mathrm{rank}(P)$), yet the classifier's reliance on that subspace---measured by $P_{\perp}$-pC and $P$-pC---remains strong and largely unchanged beyond moderate depth.  
These findings confirm our hypothesis: deeper networks learn higher-dimensional representations but retain the core simplicity bias structure described by LD-SB.

\subsection{Rich vs Lazy Regime Comparison}
\label{sec:rich_lazy}

To investigate the mechanism behind depth-dependent rank dynamics, we compare rich and lazy training regimes. The rich regime uses uniform sphere initialization enabling feature learning; the lazy regime uses NTK-style scaling where weights stay near initialization.

Figure~\ref{fig:rich_vs_lazy} shows that rank collapse is regime-dependent: in rich training, effective rank varies dramatically with depth (4.68 → 95.74), while lazy training maintains rank $\sim$96 regardless of depth. For 50-layer networks, both regimes converge to similar ranks and LD-SB metrics, suggesting deep rich networks approximate lazy behavior. The 1-layer rich model shows extreme LD-SB with $P$-pC = 0\%.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{results/plot_rich_vs_lazy.png}
\caption{Rich vs Lazy regime comparison. Rich regime shows depth-dependent rank dynamics; lazy regime maintains high rank regardless of depth.}
\label{fig:rich_vs_lazy}
\end{figure}

This reveals that depth affects LD-SB through gradient-based feature learning, not architectural capacity alone.


\section{Discussion}

Our experiments support the hypothesis that depth alters simplicity bias. The rank increase from 4.68 (1-layer) to 95.74 (50-layer) reflects weight updates being distributed across layers, preventing extreme collapse. Yet LD-SB persists: $P$-pC stays at $\sim$38\% even with high rank, showing predictions still depend on the learned subspace.

\textbf{Practical implications:} Deeper networks mitigate extreme rank collapse in rich training, but with trade-offs---accuracy peaks at 10 layers. Moderate depth (10--20 layers) may balance rank preservation and trainability.

\textbf{Future work:} Extend to end-to-end CNN training, investigate robustness correlations, and explore combining depth with debiasing methods like OrthoP \cite{b8}.

\section{Conclusion}

We investigated how network depth affects LD-SB in MLPs trained on Waterbirds. Key findings: (1) Effective rank grows from 4.68 (1-layer) to 95.74 (50-layer); (2) LD-SB metrics remain stable ($P$-pC $\sim$38\%) despite rank increase; (3) Effects saturate at moderate depth; (4) Depth effects occur only in rich training---lazy regime maintains high rank regardless of depth.

Depth reshapes but does not eliminate simplicity bias, redistributing it across higher-dimensional subspaces through gradient-based feature learning.






\begin{thebibliography}{00}

\bibitem{b1} D. Hendrycks and T. Dietterich, ``Benchmarking neural network robustness to common corruptions and perturbations,'' in \textit{Proc. Int. Conf. Learn. Representations (ICLR)}, 2019.

\bibitem{b2} H. Shah, K. Tamuly, A. Raghunathan, P. Jain, and P. Netrapalli, ``The pitfalls of simplicity bias in neural networks,'' in \textit{Advances in Neural Information Processing Systems (NeurIPS)}, vol. 33, pp. 9573--9585, 2020.

\bibitem{b3} R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F.A. Wichmann, ``Shortcut learning in deep neural networks,'' \textit{Nature Machine Intelligence}, vol. 2, no. 11, pp. 665--673, 2020.

\bibitem{b4} K. Hermann and A. Lampinen, ``What shapes feature representations? Exploring datasets, architectures, and training,'' in \textit{Advances in Neural Information Processing Systems (NeurIPS)}, vol. 33, pp. 9995--10006, 2020.

\bibitem{b5} M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz, ``Invariant risk minimization,'' \textit{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem{b6} S. Sagawa, P.W. Koh, T.B. Hashimoto, and P. Liang, ``Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization,'' in \textit{Proc. Int. Conf. Learn. Representations (ICLR)}, 2020.

\bibitem{b7} D. Morwani, J. Batra, P. Jain, and P. Netrapalli, ``Simplicity bias in 1-hidden layer neural networks,'' in \textit{Advances in Neural Information Processing Systems (NeurIPS)}, vol. 36, 2023.

\bibitem{b8} W. Liang, J. Zou, and Z. Yu, ``OrthoP: Mitigating simplicity bias via orthogonal projection,'' \textit{arXiv preprint arXiv:2210.09587}, 2022.

\end{thebibliography}

\section*{Additional Figures}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.70\linewidth]{lazy_rank.png}
\caption{Effective rank evolution in lazy regime.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.70\linewidth]{results/plot_effective_rank.png}
\caption{Effective rank vs.\ depth (rich regime).}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.70\linewidth]{results/plot_p_perp_pc.png}
\caption{$P_{\perp}$-pC vs.\ depth.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.70\linewidth]{results/plot_p_pc.png}
\caption{$P$-pC vs.\ depth.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.70\linewidth]{results/plot_val_acc.png}
\caption{Validation accuracy vs.\ depth.}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.70\linewidth]{results/plot_rank_over_time.png}
\caption{Rank evolution during training by depth.}
\end{figure}

\end{document}

